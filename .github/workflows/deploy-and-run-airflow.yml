name: Deploy and Run Airflow DAG

on:
  workflow_dispatch:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened

jobs:
  deploy-and-run:
    runs-on: ubuntu-latest

    steps:
    # 1. Checkout le code
    - name: Checkout repository
      uses: actions/checkout@v3

    # 2. Installer Docker Compose
    - name: Install Docker Compose
      run: |
        sudo apt-get update
        sudo apt-get install -y docker-compose

    - name: Changing permissions on volumes
      run: |
        sudo chmod -R 777 airflow/logs/
        sudo chmod -R 777 airflow/dags/
        sudo chmod -R 777 airflow/plugins/

    # 3. Construire et démarrer les services avec Docker Compose
    - name: Start Docker Compose Services
      run: |
        docker-compose -f docker-compose.yml up -d

    - name: Check Docker Compose Status
      run: |
        docker-compose -f docker-compose.yml ps
        docker-compose -f docker-compose.yml logs airflow-webserver

    - name: Inspect Docker Network
      run: |
        docker network ls
        docker network inspect bridge

    - name: Check permissions on volumes
      run: |
        ls -la airflow

    # 4. Attendre que les services soient prêts
    - name: Wait for Airflow Webserver
      run: |
        for i in {1..10}; do
          echo "Attempt $i: Checking if Airflow Webserver is ready..."
          if curl --fail http://localhost:8080/health; then
            echo "Airflow Webserver is ready."
            exit 0
          fi
          echo "Waiting for Airflow Webserver to be ready..."
          sleep 10
        done
        echo "Airflow Webserver did not start within the expected time."
        exit 1

    # Déclencher le DAG Airflow
    - name: Trigger Airflow DAG
      id: trigger_dag
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        DAG_RUN_ID=$(curl -s -X POST http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns \
          -H "Content-Type: application/json" \
          -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          -d '{"conf": {}}' | jq -r '.dag_run_id')
        echo "Triggered DAG Run ID: $DAG_RUN_ID"
        echo "dag_run_id=$DAG_RUN_ID" >> $GITHUB_ENV

    # Vérifier l'état initial du DAG
    - name: Check Initial State of DAG
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID | jq

    # Attendre la fin du DAG
    - name: Wait for Airflow DAG Completion
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        for i in {1..30}; do
          STATE=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID \
            | jq -r '.state')
          echo "Current state: $STATE"
          if [ "$STATE" == "success" ]; then
            echo "DAG execution succeeded."
            exit 0
          elif [ "$STATE" == "failed" ]; then
            echo "DAG execution failed."
            exit 1
          fi
          sleep 10
        done
        echo "DAG did not complete within the expected time."
        exit 1

    # Récupérer la liste des tâches du DAG
    - name: Fetch Task List
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        export DAG_ID=ml_pipeline
        TASKS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances \
          | jq -r '.task_instances[].task_id')
        echo "Tasks in DAG: $TASKS"

    # Vérifier l'état de chaque tâche
    - name: Check Task States
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        export DAG_ID=ml_pipeline
        TASKS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances \
          | jq -r '.task_instances[].task_id')
        for TASK_ID in $TASKS; do
          STATE=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances/$TASK_ID \
            | jq -r '.state')
          echo "Task $TASK_ID is in state: $STATE"
        done

    # Récupérer les logs des tâches
    - name: Fetch Task Logs
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        export DAG_ID=ml_pipeline
        TASKS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances \
          | jq -r '.task_instances[].task_id')
        for TASK_ID in $TASKS; do
          echo "Fetching logs for Task: $TASK_ID"
          LOGS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances/$TASK_ID/logs)
          echo "Logs for Task $TASK_ID:"
          echo "$LOGS"
          echo -e "\n---------------------------------------------\n"
        done
  
    # 6. Afficher les logs des conteneurs en cas de succès ou d'échec
    - name: Show Docker Compose Logs
      if: always()
      run: docker-compose -f docker-compose.yml logs