name: Deploy and Run Airflow DAG

on:
  workflow_dispatch:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened

jobs:
  deploy-and-run:
    runs-on: ubuntu-latest

    steps:
    # 1. Checkout le code
    - name: Checkout repository
      uses: actions/checkout@v3

    # 2. Installer Docker Compose
    - name: Install Docker Compose
      run: |
        sudo apt-get update
        sudo apt-get install -y docker-compose

    - name: Changing permissions on volumes
      run: |
        sudo chmod -R 777 airflow/logs/
        sudo chmod -R 777 airflow/dags/
        sudo chmod -R 777 airflow/plugins/

    # 3. Construire et démarrer les services avec Docker Compose
    - name: Start Docker Compose Services
      run: |
        docker-compose -f docker-compose.yml up -d

    - name: Check Docker Compose Status
      run: |
        docker-compose -f docker-compose.yml ps
        docker-compose -f docker-compose.yml logs airflow-webserver

    - name: Inspect Docker Network
      run: |
        docker network ls
        docker network inspect bridge

    - name: Check permissions on volumes
      run: |
        ls -la airflow

    # 4. Attendre que les services soient prêts
    - name: Wait for Airflow Webserver
      run: |
        for i in {1..10}; do
          echo "Attempt $i: Checking if Airflow Webserver is ready..."
          if curl --fail http://localhost:8080/health; then
            echo "Airflow Webserver is ready."
            exit 0
          fi
          echo "Waiting for Airflow Webserver to be ready..."
          sleep 10
        done
        echo "Airflow Webserver did not start within the expected time."
        exit 1

    # 5. Déclencher le DAG Airflow
    - name: Trigger Airflow DAG
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        curl -X POST http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns \
          -H "Content-Type: application/json" \
          -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          -d '{"conf": {}}'

    - name: Wait for Airflow DAG Execution
      run: |
        # Étape 1 : Récupérer le dernier dag_run_id pour le DAG ml_pipeline
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        DAG_RUN_ID=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns?order_by=-execution_date \
          | jq -r '.dag_runs[0].dag_run_id')
        
        echo "Last DAG Run ID: $DAG_RUN_ID"

        # Étape 2 : Boucle pour attendre que le DAG soit terminé
        for i in {1..30}; do
          echo "Checking DAG execution state..."
          STATE=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID \
            | jq -r '.state')
          echo "Current state: $STATE"
          if [ "$STATE" == "success" ]; then
            echo "DAG execution succeeded."
            exit 0
          elif [ "$STATE" == "failed" ]; then
            echo "DAG execution failed."
            exit 1
          fi
          sleep 10
        done
        echo "DAG did not complete within the expected time."
        exit 1
  
    # 6. Afficher les logs des conteneurs en cas de succès ou d'échec
    - name: Show Docker Compose Logs
      if: always()
      run: docker-compose -f docker-compose.yml logs