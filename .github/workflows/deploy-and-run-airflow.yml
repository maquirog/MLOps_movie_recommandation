name: Deploy, Debug, and Run Airflow DAG

on:
  workflow_dispatch:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened

jobs:
  deploy-and-debug:
    runs-on: ubuntu-latest
    env:
      AIRFLOW_HOST: "localhost"
      AIRFLOW_PORT: "8080"
      AIRFLOW_USERNAME: "airflow"
      AIRFLOW_PASSWORD: "airflow"
      DAG_ID: "ml_pipeline"
      LOGS_PATH: "airflow/logs"

    steps:
    # 1. Checkout le code
    - name: Checkout Repository
      uses: actions/checkout@v3

    # 2. Installer les dépendances nécessaires (jq et Docker Compose)
    - name: Install Dependencies
      run: |
        echo "Installing dependencies..."
        sudo apt-get update
        sudo apt-get install -y jq docker-compose
        echo "Dependencies installed successfully."

    # 3. Modifier les permissions sur les volumes
    - name: Change Permissions on Volumes
      run: |
        echo "Changing permissions on volumes..."
        sudo chmod -R 777 airflow/logs/
        sudo chmod -R 777 airflow/dags/
        sudo chmod -R 777 airflow/plugins/
        echo "Permissions changed successfully."

    # 4. Construire et démarrer les services avec Docker Compose
    - name: Start Docker Compose Services
      run: |
        echo "Starting Docker Compose services..."
        docker-compose -f docker-compose.yml up -d
        echo "Docker Compose services started successfully."

    # 5. Vérifier l'état des services Docker Compose
    - name: Check Docker Compose Status
      run: |
        echo "Checking Docker Compose status..."
        docker-compose -f docker-compose.yml ps
        docker-compose -f docker-compose.yml logs airflow-webserver
        echo "Docker Compose status checked."

    # 6.1 Vérification de la santé d'Airflow avec une boucle simplifiée
    - name: Wait for Airflow Webserver
      run: |
        echo "Waiting for Airflow Webserver to be ready..."
        for i in {1..20}; do
          echo "Attempt $i: Checking if Airflow Webserver is ready..."
          
          # Test la connexion avec curl --fail
          if curl --fail http://$AIRFLOW_HOST:$AIRFLOW_PORT/health; then
            echo "Airflow Webserver is ready."
            exit 0
          fi
          
          echo "Airflow Webserver is not ready yet. Retrying in 10 seconds..."
          sleep 10
        done
        
        # Si la boucle atteint la limite maximale sans succès
        echo "Airflow Webserver did not start within the expected time."
        exit 1

    # 6.2 Activer le DAG s'il est désactivé
    - name: Activate Airflow DAG
      run: |
        echo "Activating Airflow DAG..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, DAG_ID: $DAG_ID"
        RESPONSE=$(curl -v -X PATCH http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID \
          -H "Content-Type: application/json" \
          -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          -d '{"is_paused": false}')
        echo "Response from activating DAG:"
        echo "$RESPONSE" | jq
        echo "Airflow DAG activated."

    # 6.3 Déclencher le DAG
    - name: Trigger Airflow DAG
      id: trigger_dag
      run: |
        echo "Triggering Airflow DAG..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, DAG_ID: $DAG_ID"
        DAG_RUN_RESPONSE=$(curl -v -X POST http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns \
          -H "Content-Type: application/json" \
          -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          -d '{"conf": {}}')
        echo "DAG Run Response:"
        echo "$DAG_RUN_RESPONSE"
        DAG_RUN_ID=$(echo $DAG_RUN_RESPONSE | jq -r '.dag_run_id')
        echo "Extracted DAG_RUN_ID: $DAG_RUN_ID"
        echo "DAG_RUN_ID=$DAG_RUN_ID" >> $GITHUB_ENV
        echo "Airflow DAG triggered with DAG_RUN_ID=$DAG_RUN_ID."

    # 6.4 Attendre que le DAG Run ne soit plus "running"
    - name: Wait for DAG Run to Complete
      run: |
        echo "Waiting for DAG Run to complete..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, DAG_RUN_ID: $DAG_RUN_ID"
        while true; do
          DAG_RUN_STATUS=$(curl -v -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID | jq -r '.state')
          echo "Current DAG Run Status: $DAG_RUN_STATUS"
          if [[ "$DAG_RUN_STATUS" != "running" ]]; then
            break
          fi
          sleep 10
        done
        echo "DAG Run completed with status: $DAG_RUN_STATUS."

    # 6.5 Récupérer les tâches associées au DAG Run
    - name: Fetch Tasks for DAG Run
      id: fetch_tasks
      run: |
        echo "Fetching tasks for DAG Run..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, DAG_RUN_ID: $DAG_RUN_ID"

        # Récupérer les tâches et les convertir en une chaîne séparée par des virgules
        TASKS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances | jq -r '.task_instances[].task_id' | tr '\n' ',')

        # Nettoyer la dernière virgule (si nécessaire)
        TASKS=${TASKS%,}

        echo "Retrieved tasks:"
        echo "$TASKS"

        # Ajouter les tâches comme variable d'environnement
        echo "TASKS=$TASKS" >> $GITHUB_ENV

    # 6.6 Récupérer les logs et statuts des tâches
    - name: Fetch Logs and Status for Tasks
      run: |
        echo "Fetching logs and status for tasks..."
        echo "Using TASKS: $TASKS"
        for TASK_ID in $TASKS; do
          echo "Processing Task: $TASK_ID"
          TASK_STATUS=$(curl -v -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances/$TASK_ID | jq -r '.state')
          echo "Task Status: $TASK_STATUS"
          LOG_DIR="$LOGS_PATH/dag_id=$DAG_ID/run_id=$DAG_RUN_ID/task_id=$TASK_ID"
          echo "Checking logs in directory: $LOG_DIR"
          if [ -d "$LOG_DIR" ]; then
            echo "Logs Directory: $LOG_DIR"
            find "$LOG_DIR" -type f -exec echo "Log File: {}" \; -exec cat {} \;
          else
            echo "Logs Directory Not Found for Task: $TASK_ID"
          fi
        done
        echo "Logs and status fetched for all tasks."

    # 7. Afficher les logs des conteneurs Docker
    - name: Show Docker Compose Logs
      if: always()
      run: |
        echo "Showing Docker Compose logs..."
        docker-compose -f docker-compose.yml logs
        echo "Docker Compose logs displayed."