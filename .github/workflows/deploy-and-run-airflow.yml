name: Deploy, Debug, and Run Airflow DAG

on:
  workflow_dispatch:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened

jobs:
  deploy-and-debug:
    runs-on: ubuntu-latest

    steps:
    # 1. Checkout le code
    - name: Checkout Repository
      uses: actions/checkout@v3

    # 2. Installer Docker Compose
    - name: Install Docker Compose
      run: |
        sudo apt-get update
        sudo apt-get install -y docker-compose

    # 3. Modifier les permissions sur les volumes
    - name: Change Permissions on Volumes
      run: |
        sudo chmod -R 777 airflow/logs/
        sudo chmod -R 777 airflow/dags/
        sudo chmod -R 777 airflow/plugins/

    # 4. Construire et démarrer les services avec Docker Compose
    - name: Start Docker Compose Services
      run: |
        docker-compose -f docker-compose.yml up -d

    # 5. Vérifier l'état des services Docker Compose
    - name: Check Docker Compose Status
      run: |
        docker-compose -f docker-compose.yml ps
        docker-compose -f docker-compose.yml logs airflow-webserver

    # 6. Inspecter le réseau Docker
    - name: Inspect Docker Network
      run: |
        docker network ls
        docker network inspect bridge

    # 7. Vérifier les permissions sur les volumes
    - name: Check Permissions on Volumes
      run: |
        ls -la airflow
        ls -la airflow/logs
        ls -la airflow/dags
        ls -la airflow/plugins

    # 8. Vérifier les conteneurs Docker actifs
    - name: List Running Docker Containers
      run: |
        docker ps
        docker ps -a

    # 9. Attendre que le serveur Airflow soit prêt
    - name: Wait for Airflow Webserver
      run: |
        for i in {1..20}; do
          echo "Attempt $i: Checking if Airflow Webserver is ready..."
          if curl --fail http://localhost:8080/health; then
            echo "Airflow Webserver is ready."
            exit 0
          fi
          echo "Waiting for Airflow Webserver to be ready..."
          sleep 10
        done
        echo "Airflow Webserver did not start within the expected time."
        exit 1

    # 10. Vérifier l'état du Scheduler
    - name: Check Scheduler Status
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/health | jq '.scheduler'

    # 11. Vérifier l'état de la base de données Metadata
    - name: Check Metadata Database Health
      run: |
        curl -s http://localhost:8080/health | jq '.metadatabase'

    # 12. Lister les DAGs disponibles
    - name: List Available DAGs
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags | jq

    # 13. Vérifier si le DAG est activé
    - name: Check DAG Activation
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/ml_pipeline | jq

    # 14. Déclencher le DAG Airflow
    - name: Trigger Airflow DAG
      id: trigger_dag
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        DAG_RUN_ID=$(curl -s -X POST http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns \
          -H "Content-Type: application/json" \
          -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          -d '{"conf": {}}' | jq -r '.dag_run_id')
        echo "Triggered DAG Run ID: $DAG_RUN_ID"
        echo "dag_run_id=$DAG_RUN_ID" >> $GITHUB_ENV

    # 15. Vérifier l'état initial du DAG Run
    - name: Check Initial DAG Run State
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID | jq

    # 16. Suivre l'état du DAG Run
    - name: Monitor DAG Run State
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        for i in {1..30}; do
          STATE=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID \
            | jq -r '.state')
          echo "Current state: $STATE"
          if [ "$STATE" == "success" ]; then
            echo "DAG execution succeeded."
            exit 0
          elif [ "$STATE" == "failed" ]; then
            echo "DAG execution failed."
            exit 1
          fi
          sleep 10
        done
        echo "DAG did not complete within the expected time."
        exit 1

    # 17. Lister les tâches associées au DAG Run
    - name: List Task Instances
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID/taskInstances | jq

    # 18. Vérifier l'état de chaque tâche
    - name: Check Task States
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        TASKS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID/taskInstances \
          | jq -r '.task_instances[] | "\(.task_id):\(.state)"')
        for TASK in $TASKS; do
          TASK_ID=$(echo $TASK | cut -d':' -f1)
          TASK_STATE=$(echo $TASK | cut -d':' -f2)
          echo "Task $TASK_ID is in state: $TASK_STATE"
        done

    # 19. Récupérer les logs des tâches
    - name: Fetch Task Logs
      run: |
        export AIRFLOW_USERNAME=airflow
        export AIRFLOW_PASSWORD=airflow
        export DAG_RUN_ID=$DAG_RUN_ID
        TASKS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID/taskInstances \
          | jq -r '.task_instances[].task_id')
        for TASK_ID in $TASKS; do
          echo "Fetching logs for Task: $TASK_ID"
          LOGS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID/taskInstances/$TASK_ID/logs)
          echo "Logs for Task $TASK_ID:"
          echo "$LOGS"
          echo -e "\n---------------------------------------------\n"
        done

    # 20. Afficher les logs des conteneurs Docker
    - name: Show Docker Compose Logs
      if: always()
      run: docker-compose -f docker-compose.yml logs