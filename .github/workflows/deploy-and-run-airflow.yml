name: Deploy, Debug, and Run Airflow DAG

on:
  workflow_dispatch:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened

jobs:
  deploy-and-debug:
    runs-on: ubuntu-latest
    steps:
    # 1. Checkout le code
    - name: Checkout Repository
      uses: actions/checkout@v3

    # 2. Installer les dépendances nécessaires (jq et Docker Compose)
    - name: Install Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y jq docker-compose

    # 3. Modifier les permissions sur les volumes
    - name: Change Permissions on Volumes
      run: |
        sudo chmod -R 777 airflow/logs/
        sudo chmod -R 777 airflow/dags/
        sudo chmod -R 777 airflow/plugins/

    # 4. Construire et démarrer les services avec Docker Compose
    - name: Start Docker Compose Services
      run: |
        docker-compose -f docker-compose.yml up -d

    # 5. Vérifier l'état des services Docker Compose
    - name: Check Docker Compose Status
      run: |
        docker-compose -f docker-compose.yml ps
        docker-compose -f docker-compose.yml logs airflow-webserver

    # 6.1 Vérification de la santé d'Airflow
    - name: Check Airflow Health
      run: |
        curl -s http://localhost:8080/health | jq

    # 6.2 Activer le DAG s'il est désactivé
    - name: Activate Airflow DAG
      run: |
        curl -s -X PATCH http://localhost:8080/api/v1/dags/ml_pipeline \
          -H "Content-Type: application/json" \
          -u "airflow:airflow" \
          -d '{"is_paused": false}' | jq

    # 6.3 Déclencher le DAG
    - name: Trigger Airflow DAG
      id: trigger_dag
      run: |
        DAG_RUN_RESPONSE=$(curl -s -X POST http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns \
          -H "Content-Type: application/json" \
          -u "airflow:airflow" \
          -d '{"conf": {}}')
        echo "DAG Run Response: $DAG_RUN_RESPONSE"
        DAG_RUN_ID=$(echo $DAG_RUN_RESPONSE | jq -r '.dag_run_id')
        echo "DAG_RUN_ID=$DAG_RUN_ID" >> $GITHUB_ENV

    # 6.4 Attendre que le DAG Run ne soit plus "running"
    - name: Wait for DAG Run to Complete
      run: |
        while true; do
          DAG_RUN_STATUS=$(curl -s -u "airflow:airflow" \
            http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID | jq -r '.state')
          echo "Current DAG Run Status: $DAG_RUN_STATUS"
          if [[ "$DAG_RUN_STATUS" != "running" ]]; then
            break
          fi
          sleep 10
        done

    # 6.5 Récupérer les tâches associées au DAG Run
    - name: Fetch Tasks for DAG Run
      id: fetch_tasks
      run: |
        TASKS=$(curl -s -u "airflow:airflow" \
          http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID/taskInstances | jq -r '.task_instances[].task_id')
        echo "TASKS=$TASKS" >> $GITHUB_ENV

    # 6.6 Récupérer les logs et statuts des tâches
    - name: Fetch Logs and Status for Tasks
      run: |
        for TASK_ID in ${{ env.TASKS }}; do
          echo "Processing Task: $TASK_ID"
          TASK_STATUS=$(curl -s -u "airflow:airflow" \
            http://localhost:8080/api/v1/dags/ml_pipeline/dagRuns/$DAG_RUN_ID/taskInstances/$TASK_ID | jq -r '.state')
          echo "Task Status: $TASK_STATUS"
          LOG_DIR="airflow/logs/dag_id=ml_pipeline/run_id=$DAG_RUN_ID/task_id=$TASK_ID"
          if [ -d "$LOG_DIR" ]; then
            echo "Logs Directory: $LOG_DIR"
            find "$LOG_DIR" -type f -exec echo "Log File: {}" \; -exec cat {} \;
          else
            echo "Logs Directory Not Found for Task: $TASK_ID"
          fi
        done

    # 7. Afficher les logs des conteneurs Docker
    - name: Show Docker Compose Logs
      if: always()
      run: docker-compose -f docker-compose.yml logs