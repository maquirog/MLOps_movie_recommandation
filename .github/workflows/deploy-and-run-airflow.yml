name: Deploy, Debug, and Run Airflow DAG

on:
  workflow_dispatch:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened

jobs:
  deploy-and-debug:
    runs-on: ubuntu-latest
    env:
      AIRFLOW_HOST: "localhost"
      AIRFLOW_PORT: "8080"
      AIRFLOW_USERNAME: "airflow"
      AIRFLOW_PASSWORD: "airflow"
      DAG_ID: "ml_pipeline"
      LOGS_PATH: "airflow/logs"

    steps:
    # 1. Checkout le code
    - name: Checkout Repository
      uses: actions/checkout@v3

    # 2. Installer les dépendances nécessaires (jq et Docker Compose)
    - name: Install Dependencies
      run: |
        echo "Installing dependencies..."
        sudo apt-get update
        sudo apt-get install -y tree docker-compose
        echo "Dependencies installed successfully."

    # 3. Modifier les permissions sur les volumes
    - name: Change Permissions on Volumes
      run: |
        echo "Changing permissions on volumes..."
        sudo chmod -R 777 airflow/logs/
        sudo chmod -R 777 airflow/dags/
        sudo chmod -R 777 airflow/plugins/
        echo "Permissions changed successfully."

    - name: Print Current Folder Arborescence
      run: |
        echo "Current folder structure:"
        tree || (echo "'tree' command not found, using 'find' as a fallback:" && find .)

    # 4. Construire et démarrer les services avec Docker Compose
    - name: Start Docker Compose Services
      run: |
        echo "Retrieving Docker Group ID..."
        DOCKER_GID=$(getent group docker | cut -d: -f3)
        AIRFLOW_UID=1000
        AIRFLOW_GID=0
        echo "DOCKER_GID=$DOCKER_GID" > .env
        echo "AIRFLOW_UID=$AIRFLOW_UID" >> .env
        echo "AIRFLOW_GID=$AIRFLOW_GID" >> .env
    
        echo "Starting Docker Compose services..."
        docker-compose -f docker-compose.yml --env-file .env up -d
        echo "Docker Compose services started successfully."

    # 5. Vérifier l'état des services Docker Compose
    - name: Check Docker Compose Status
      run: |
        echo "Checking Docker Compose status..."
        docker-compose -f docker-compose.yml ps
        docker-compose -f docker-compose.yml logs airflow-webserver
        echo "Docker Compose status checked."

    # 6.1 Vérification de la santé d'Airflow avec une boucle simplifiée
    - name: Wait for Airflow Webserver
      run: |
        echo "Waiting for Airflow Webserver to be ready..."
        for i in {1..20}; do
          echo "Attempt $i: Checking if Airflow Webserver is ready..."
          
          # Test la connexion avec curl --fail
          if curl --fail http://$AIRFLOW_HOST:$AIRFLOW_PORT/health; then
            echo "Airflow Webserver is ready."
            exit 0
          fi
          
          echo "Airflow Webserver is not ready yet. Retrying in 10 seconds..."
          sleep 10
        done
        
        # Si la boucle atteint la limite maximale sans succès
        echo "Airflow Webserver did not start within the expected time."
        exit 1

    # 6.2 Activer le DAG s'il est désactivé
    - name: Activate Airflow DAG
      run: |
        echo "Activating Airflow DAG..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, DAG_ID: $DAG_ID"
        RESPONSE=$(curl -v -X PATCH http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID \
          -H "Content-Type: application/json" \
          -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          -d '{"is_paused": false}')
        echo "Response from activating DAG:"
        echo "$RESPONSE" | jq
        echo "Airflow DAG activated."

    # 6.3 Déclencher le DAG
    - name: Trigger Airflow DAG
      id: trigger_dag
      run: |
        echo "Triggering Airflow DAG..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, DAG_ID: $DAG_ID"
        DAG_RUN_RESPONSE=$(curl -v -X POST http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns \
          -H "Content-Type: application/json" \
          -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          -d '{"conf": {}}')
        echo "DAG Run Response:"
        echo "$DAG_RUN_RESPONSE"
        DAG_RUN_ID=$(echo $DAG_RUN_RESPONSE | jq -r '.dag_run_id')
        echo "Extracted DAG_RUN_ID: $DAG_RUN_ID"
        echo "DAG_RUN_ID=$DAG_RUN_ID" >> $GITHUB_ENV
        echo "Airflow DAG triggered with DAG_RUN_ID=$DAG_RUN_ID."

    # 6.4 Attendre que le DAG Run ne soit plus "running"
    - name: Debug DAG Run and Fetch Logs from Current Folder
      run: |
        echo "Starting DAG Run Debugging and Log Fetching..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, AIRFLOW_LOGS_PATH: $AIRFLOW_LOGS_PATH"
        echo "DAG_ID: $DAG_ID"
    
        # Fetch all DAG_RUN_IDs for the given DAG_ID
        echo "Fetching DAG_RUN_IDs..."
        DAG_RUNS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns || { echo "Failed to fetch DAG_RUN_IDs"; exit 1; })
        
        echo "Available DAG_RUN_IDs:"
        echo "$DAG_RUNS" | jq -r '.dag_runs[].dag_run_id' || { echo "Failed to parse DAG_RUN_IDs"; exit 1; }
    
        # Select the latest DAG_RUN_ID dynamically
        DAG_RUN_ID=$(echo "$DAG_RUNS" | jq -r '.dag_runs | sort_by(.execution_date) | last(.[]).dag_run_id' || { echo "Failed to select DAG_RUN_ID"; exit 1; })
        echo "Latest DAG_RUN_ID: $DAG_RUN_ID"
    
        MAX_RETRIES=20
        RETRY_COUNT=0
        
        while true; do
          # Increment the retry counter
          RETRY_COUNT=$((RETRY_COUNT + 1))
        
          # Fetch DAG run status
          echo "Fetching DAG Run status..."
          FULL_RESPONSE=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID || { echo "Failed to fetch DAG run status"; exit 1; })
          DAG_RUN_STATUS=$(echo "$FULL_RESPONSE" | jq -r '.state' || { echo "Failed to parse DAG run status"; exit 1; })
          echo "Attempt $RETRY_COUNT: Current DAG Run Status for DAG_RUN_ID $DAG_RUN_ID: $DAG_RUN_STATUS"
        
          # Fetch task instance statuses
          echo "Fetching task instance statuses..."
          TASK_INSTANCES=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances || { echo "Failed to fetch task instance statuses"; exit 1; })
          echo "Task Instance Statuses: $TASK_INSTANCES"
        
          # Iterate over each task instance
          echo "$TASK_INSTANCES" | jq -c '.task_instances[]' | while read -r task_instance; do
            TASK_ID=$(echo "$task_instance" | jq -r '.task_id' || { echo "Failed to parse TASK_ID"; exit 1; })
            TASK_STATE=$(echo "$task_instance" | jq -r '.state' || { echo "Failed to parse TASK_STATE"; exit 1; })
            echo "Task ID: $TASK_ID, State: $TASK_STATE"
        
            if [[ "$TASK_STATE" == "up_for_retry" || "$TASK_STATE" == "failed" ]]; then
              echo "Fetching logs directly from volume for Task ID: $TASK_ID"
              
              # Path to Airflow logs
              echo "Resolving absolute log path..."
              ABSOLUTE_LOG_PATH=$(cd "$AIRFLOW_LOGS_PATH/dag_id=$DAG_ID/run_id=$DAG_RUN_ID/task_id=$TASK_ID/" && pwd)
              echo "Absolute Log Path: $ABSOLUTE_LOG_PATH"
              
              # Search for the logs
              LOG_FILE=$(find "$AIRFLOW_LOGS_PATH" -type f -path "*/dag_id=$DAG_ID/run_id=$DAG_RUN_ID/task_id=$TASK_ID/*log" 2>/dev/null || { echo "Failed to search for log files"; exit 1; })
              
              if [[ -n "$LOG_FILE" ]]; then
                echo "Log file found: $LOG_FILE"
                cat "$LOG_FILE"
              else
                echo "No log file found for Task ID: $TASK_ID"
              fi
            fi
          done
        
          # Break the loop if the DAG run is complete
          if [[ "$DAG_RUN_STATUS" != "running" && "$DAG_RUN_STATUS" != "queued" ]]; then
            break
          fi
        
          # Break the loop if the retry count exceeds the maximum retries
          if [[ $RETRY_COUNT -ge $MAX_RETRIES ]]; then
            echo "Maximum retries reached. Exiting."
            exit 1
          fi
        
          # Wait 10 seconds before retrying
          sleep 10
        done
        
        echo "DAG Run completed with status: $DAG_RUN_STATUS."
      env:
        AIRFLOW_LOGS_PATH: "airflow/logs"
        DAG_ID: "ml_pipeline"
        AIRFLOW_HOST: "localhost"
        AIRFLOW_PORT: "8080"

    - name: Print Current Folder Arborescence
      if: always()
      run: |
        echo "Current folder structure:"
        tree || (echo "'tree' command not found, using 'find' as a fallback:" && find .)

    # 6.5 Récupérer les tâches associées au DAG Run
    - name: Fetch Tasks for DAG Run
      id: fetch_tasks
      if: always()
      run: |
        echo "Fetching tasks for DAG Run..."
        echo "Using AIRFLOW_HOST: $AIRFLOW_HOST, AIRFLOW_PORT: $AIRFLOW_PORT, DAG_RUN_ID: $DAG_RUN_ID"

        # Récupérer les tâches et les convertir en une liste séparée par des espaces
        TASKS=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
          http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances | jq -r '.task_instances[].task_id' | tr '\n' ' ')

        echo "Retrieved tasks:"
        echo "$TASKS"

        # Ajouter les tâches comme variable d'environnement
        echo "TASKS=$TASKS" >> $GITHUB_ENV

    # 6.6 Vérifier le contenu du répertoire des logs du DAG
    - name: Check Logs Directory
      if: always()
      run: |
        echo "Checking logs directory for DAG ID: $DAG_ID..."
        ls -la airflow/logs/
        echo "Logs directory content listed successfully."

    # 6.7 Récupérer les logs et statuts des tâches
    - name: Fetch Logs and Status for Tasks
      if: always()
      run: |
        echo "Fetching logs and status for tasks..."
        echo "Using TASKS: $TASKS"
        for TASK_ID in $TASKS; do
          echo "Processing Task: $TASK_ID"
          TASK_STATUS=$(curl -v -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
            http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID/taskInstances/$TASK_ID | jq -r '.state')
          echo "Task Status: $TASK_STATUS"
          LOG_DIR="$LOGS_PATH/dag_id=$DAG_ID/run_id=$DAG_RUN_ID/task_id=$TASK_ID"
          echo "Checking logs in directory: $LOG_DIR"
          if [ -d "$LOG_DIR" ]; then
            echo "Logs Directory: $LOG_DIR"
            find "$LOG_DIR" -type f -exec echo "Log File: {}" \; -exec cat {} \;
          else
            echo "Logs Directory Not Found for Task: $TASK_ID"
          fi
        done
        echo "Logs and status fetched for all tasks."

    # 7. Afficher les logs des conteneurs Docker
    - name: Show Docker Compose Logs
      if: always()
      run: |
        echo "Showing Docker Compose logs..."
        docker-compose -f docker-compose.yml logs
        echo "Docker Compose logs displayed."